{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"*****\"\n",
    "mycsv = pd.read_csv(input_data)\n",
    "mycsv[\"fn\"] = [\"../jpegs_same_contrast_cropped/\" + x.split(\"/\")[-1].split(\"-\")[-1] for x in mycsv.image]\n",
    "mycsv = mycsv.loc[mycsv[\"choice\"].isin([\"Codman Hakim\", \"Codman Certas Plus\", \"Sophysa Sophy SM8\", \"proGAV 2.0\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"splits.pkl\", \"rb\") as f:\n",
    "    splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(s: int): return f'resnet34_pretrained_4_ventile_on_patient_split_squish_pret_{s}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dls(split, bs=32):\n",
    "    dblock = DataBlock(\n",
    "                        blocks=(ImageBlock, CategoryBlock(\n",
    "                            vocab=[\"Codman Hakim\", \"Codman Certas Plus\", \"Sophysa Sophy SM8\", \"proGAV 2.0\"],\n",
    "                            sort=False\n",
    "                        )), \n",
    "                        get_x=ColReader(\"fn\"),\n",
    "                        get_y=ColReader(\"choice\"),\n",
    "                        splitter=IndexSplitter(split[1]),\n",
    "                        item_tfms=[Resize(512)],\n",
    "                        batch_tfms=aug_transforms()\n",
    "    )\n",
    "    dsets = dblock.datasets(mycsv)\n",
    "    dls = dblock.dataloaders(mycsv, bs=bs)\n",
    "    return dls\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    resnet34 = torchvision.models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "    body = create_body(resnet34, cut=-2)\n",
    "    head = create_head(512, 4)\n",
    "    return nn.Sequential(body, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_prediction_metrics(softmax_probs):\n",
    "    \"\"\"\n",
    "    Calculate various prediction confidence metrics.\n",
    "    \n",
    "    Args:\n",
    "        softmax_probs (torch.Tensor): Tensor of shape [n, 4] containing softmax probabilities\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (entropy, max_prob, prob_gap) tensors of shape [n]\n",
    "    \"\"\"\n",
    "    # Calculate entropy\n",
    "    eps = 1e-15\n",
    "    softmax_probs_clipped = torch.clamp(softmax_probs, min=eps, max=1.0)\n",
    "    entropy = -torch.sum(softmax_probs_clipped * torch.log2(softmax_probs_clipped), dim=1)\n",
    "    \n",
    "    # Get top 2 probabilities for each prediction\n",
    "    top2_probs, _ = torch.topk(softmax_probs, k=2, dim=1)\n",
    "    \n",
    "    # Maximum probability\n",
    "    max_prob = top2_probs[:, 0]\n",
    "    \n",
    "    # Gap between top 2 probabilities\n",
    "    prob_gap = top2_probs[:, 0] - top2_probs[:, 1]\n",
    "    \n",
    "    return entropy, max_prob, prob_gap\n",
    "\n",
    "def analyze_metrics_by_target(softmax_probs, targets):\n",
    "    \"\"\"\n",
    "    Calculate and analyze prediction metrics grouped by target class.\n",
    "    \n",
    "    Args:\n",
    "        softmax_probs (torch.Tensor): Tensor of shape [n, 4] containing softmax probabilities\n",
    "        targets (torch.Tensor): Tensor of shape [n] containing target classes\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing metric statistics for each class\n",
    "    \"\"\"\n",
    "    entropy, max_prob, prob_gap = calculate_prediction_metrics(softmax_probs)\n",
    "    unique_targets = torch.unique(targets)\n",
    "    \n",
    "    metrics_by_class = {}\n",
    "    for target in unique_targets:\n",
    "        mask = (targets == target)\n",
    "        class_entropy = entropy[mask]\n",
    "        class_max_prob = max_prob[mask]\n",
    "        class_prob_gap = prob_gap[mask]\n",
    "        \n",
    "        metrics_by_class[target.item()] = {\n",
    "            'entropy': {\n",
    "                'mean': class_entropy.mean().item(),\n",
    "                'std': class_entropy.std().item(),\n",
    "                'min': class_entropy.min().item(),\n",
    "                'max': class_entropy.max().item(),\n",
    "                'values': class_entropy\n",
    "            },\n",
    "            'max_prob': {\n",
    "                'mean': class_max_prob.mean().item(),\n",
    "                'std': class_max_prob.std().item(),\n",
    "                'min': class_max_prob.min().item(),\n",
    "                'max': class_max_prob.max().item(),\n",
    "                'values': class_max_prob\n",
    "            },\n",
    "            'prob_gap': {\n",
    "                'mean': class_prob_gap.mean().item(),\n",
    "                'std': class_prob_gap.std().item(),\n",
    "                'min': class_prob_gap.min().item(),\n",
    "                'max': class_prob_gap.max().item(),\n",
    "                'values': class_prob_gap\n",
    "            },\n",
    "            'count': mask.sum().item()\n",
    "        }\n",
    "    \n",
    "    return metrics_by_class\n",
    "\n",
    "def aggregate_metrics_across_splits(splits_data):\n",
    "    \"\"\"\n",
    "    Aggregate prediction metrics across multiple splits.\n",
    "    \n",
    "    Args:\n",
    "        splits_data: List of tuples, each containing (softmax_probs, targets) for a split\n",
    "        \n",
    "    Returns:\n",
    "        dict: Aggregated statistics across all splits\n",
    "    \"\"\"\n",
    "    all_metrics_by_class = defaultdict(lambda: defaultdict(list))\n",
    "    aggregated_stats = {}\n",
    "    \n",
    "    # Process each split\n",
    "    for split_idx, split in enumerate(splits_data):\n",
    "        dls = create_dls(split)\n",
    "        model = create_model()\n",
    "\n",
    "        learn = Learner(dls, model).load(get_model_name(split_idx))\n",
    "        \n",
    "        preds, targets = learn.get_preds() \n",
    "        split_stats = analyze_metrics_by_target(preds, targets)\n",
    "        \n",
    "        # Collect metrics by class across splits\n",
    "        for class_idx, stats in split_stats.items():\n",
    "            for metric_name in ['entropy', 'max_prob', 'prob_gap']:\n",
    "                all_metrics_by_class[class_idx][metric_name].append({\n",
    "                    'mean': stats[metric_name]['mean'],\n",
    "                    'std': stats[metric_name]['std'],\n",
    "                    'values': stats[metric_name]['values'],\n",
    "                    'count': stats['count'],\n",
    "                    'split_idx': split_idx\n",
    "                })\n",
    "    \n",
    "    # Calculate aggregate statistics for each class\n",
    "    for class_idx, metrics in all_metrics_by_class.items():\n",
    "        aggregated_stats[class_idx] = {}\n",
    "        \n",
    "        for metric_name, split_stats_list in metrics.items():\n",
    "            # Concatenate all values for this metric across splits\n",
    "            all_values = torch.cat([stats['values'] for stats in split_stats_list])\n",
    "            \n",
    "            # Calculate split-level statistics\n",
    "            split_means = torch.tensor([stats['mean'] for stats in split_stats_list])\n",
    "            \n",
    "            aggregated_stats[class_idx][metric_name] = {\n",
    "                'overall_mean': all_values.mean().item(),\n",
    "                'overall_std': all_values.std().item(),\n",
    "                'overall_min': all_values.min().item(),\n",
    "                'overall_max': all_values.max().item(),\n",
    "                'mean_std_error': (split_means.std() / torch.sqrt(torch.tensor(len(split_means)))).item(),\n",
    "                'per_split_means': [stats['mean'] for stats in split_stats_list],\n",
    "                'per_split_stds': [stats['std'] for stats in split_stats_list]\n",
    "            }\n",
    "        \n",
    "        # Add sample count information\n",
    "        aggregated_stats[class_idx]['total_samples'] = sum(\n",
    "            metrics['entropy'][i]['count'] for i in range(len(splits_data))\n",
    "        )\n",
    "        aggregated_stats[class_idx]['per_split_counts'] = [\n",
    "            metrics['entropy'][i]['count'] for i in range(len(splits_data))\n",
    "        ]\n",
    "    \n",
    "    return aggregated_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = aggregate_metrics_across_splits(splits)\n",
    "    \n",
    "# Print results\n",
    "print(\"\\nAggregated analysis across splits:\")\n",
    "for class_idx, stats in aggregated_results.items():\n",
    "    print(f\"\\nClass {class_idx} (Total samples: {stats['total_samples']}):\")\n",
    "    \n",
    "    for metric in ['entropy', 'max_prob', 'prob_gap']:\n",
    "        metric_stats = stats[metric]\n",
    "        print(f\"\\n  {metric.upper()}:\")\n",
    "        print(f\"    Mean: {metric_stats['overall_mean']:.3f} Â± {metric_stats['mean_std_error']:.3f}\")\n",
    "        print(f\"    Overall std: {metric_stats['overall_std']:.3f}\")\n",
    "        print(f\"    Range: [{metric_stats['overall_min']:.3f}, {metric_stats['overall_max']:.3f}]\")\n",
    "        print(f\"    Per-split means: {[f'{x:.3f}' for x in metric_stats['per_split_means']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "results = []\n",
    "reports = []\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    dls = create_dls(split)\n",
    "    model = create_model()\n",
    "\n",
    "    learn = Learner(dls, model).load(get_model_name(i))\n",
    "    \n",
    "    preds, targets = learn.get_preds()\n",
    "    predicted_labels = preds.argmax(dim=1)\n",
    "    print(\"split \", i+1, \"\\n\")\n",
    "    reports.append(classification_report(targets, predicted_labels, output_dict=True))\n",
    "    results.append({\n",
    "        'precision': precision_score(targets, predicted_labels, average='macro'),\n",
    "        'recall': recall_score(targets, predicted_labels, average='macro'),\n",
    "        'f1_score': f1_score(targets, predicted_labels, average='macro'),\n",
    "        'accuracy': accuracy_score(targets, predicted_labels),\n",
    "    })\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(map(lambda x: x['f1_score'], results))).mean().round(2), np.array(list(map(lambda x: x['accuracy'], results))).std().round(2)\n",
    "for metric in ['precision', 'recall', 'f1_score', 'accuracy']:\n",
    "    print(np.array(list(map(lambda x: x[metric], results))).mean().round(2), np.array(list(map(lambda x: x[metric], results))).std().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(metric):\n",
    "    print(metric, \":\", np.array([r[metric] for r in results]).mean().round(2), \" +- \", np.array([r[metric] for r in results]).std().round(2))\n",
    "\n",
    "for metric in ['precision', 'recall', 'f1_score', 'accuracy']:\n",
    "    print_result(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = {}\n",
    "\n",
    "for r in reports:\n",
    "    for o in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "        if o not in rs:\n",
    "            rs[o] = {}\n",
    "        for m in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            if m not in rs[o]:\n",
    "                rs[o][m] = []\n",
    "            rs[o][m].append(r[o][m])\n",
    "\n",
    "for cl in rs:\n",
    "    print(\"Class: \", cl)\n",
    "    for metric in rs[cl]:\n",
    "        print(metric, np.mean(rs[cl][metric]).round(2), \" +- \", np.std(rs[cl][metric]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfms = []\n",
    "\n",
    "for split in range(0,5):\n",
    "    dls = create_dls(splits[split])\n",
    "    model = create_model()\n",
    "\n",
    "    learn = Learner(dls, model).load(get_model_name(split))\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    cfms.append(interp.confusion_matrix())\n",
    "\n",
    "overall_cfm = np.array(cfms).mean(axis=0)\n",
    "normalized_cfm = overall_cfm.astype('float') / overall_cfm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "vocab = interp.vocab\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(normalized_cfm, interpolation='nearest', cmap=\"Blues\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "tick_marks = np.arange(len(vocab))\n",
    "plt.xticks(tick_marks, vocab, rotation=90)\n",
    "plt.yticks(tick_marks, vocab, rotation=0)\n",
    "\n",
    "thresh = normalized_cfm.max() / 2.\n",
    "for i, j in itertools.product(range(normalized_cfm.shape[0]), range(normalized_cfm.shape[1])):\n",
    "    coeff = f'{normalized_cfm[i, j]:.{2}f}'\n",
    "    plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\"\n",
    "                if normalized_cfm[i, j] > thresh else \"black\")\n",
    "\n",
    "ax = fig.gca()\n",
    "ax.set_ylim(len(vocab)-.5,-.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.grid(False)\n",
    "\n",
    "#save it as pdf\n",
    "\n",
    "plt.savefig(\"confusion_matrix_ventile.jpeg\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('confusion_matrix.pdf', format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
